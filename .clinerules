# Singaporean-Accent TTS MVP - Cline Development Rules

## 🎯 Project Overview
This project builds a minimal viable product for generating English speech with a Singaporean accent using XTTS v2 (primary) and Tortoise-TTS (fallback), with a Gradio UI and evaluation framework. The system uses few-shot voice cloning from reference audio clips to achieve accent authenticity.

## 📁 File Organization

### Directory Structure
```
accent-tts/
├── src/
│   ├── data/
│   │   ├── __init__.py
│   │   ├── ingest.py              # Audio validation & copying to data/raw
│   │   ├── prepare.py             # Resampling, normalization, segmentation
│   │   ├── transcribe.py          # Whisper transcription pipeline
│   │   ├── enhanced_processor.py  # Enhanced pipeline orchestrator
│   │   ├── diarization.py         # Speaker separation (pyannote.audio)
│   │   ├── vad.py                 # Voice activity detection (silero-vad)
│   │   ├── quality_filter.py      # Quality metrics & filtering
│   │   └── segmentation.py        # Smart segmentation on pauses
│   ├── tts/
│   │   ├── __init__.py
│   │   ├── xtts_infer.py      # XTTS v2 inference engine (primary)
│   │   └── tortoise_infer.py  # Tortoise-TTS fallback system
│   ├── ui/
│   │   ├── __init__.py
│   │   └── app.py             # Gradio web interface
│   └── eval/
│       └── checklist.md       # Evaluation rubric and phoneme list
├── data/                      # Gitignored - never commit audio files
│   ├── raw/
│   │   └── refs/             # Reference Singaporean-accent clips
│   └── processed/
│       ├── segments/         # Processed audio segments (16kHz mono)
│       └── transcripts/      # Generated transcriptions (.jsonl)
├── outputs/                   # Generated audio files
├── config/                    # Configuration files
│   └── processing_config.yaml # Enhanced pipeline configuration
├── tests/                     # Unit and integration tests
├── docs/                      # Additional documentation
├── scripts/                   # Utility and setup scripts
├── notebooks/                 # Optional exploration (Jupyter)
├── requirements.txt           # Python dependencies
├── Makefile                   # Automation commands
├── .env.example              # Environment variable template
├── .gitignore                # Comprehensive exclusions
└── README.md                 # Setup and usage documentation
```

### File Naming Conventions
- Python files: `snake_case.py`
- Configuration files: `lowercase.json`, `lowercase.yaml`
- Documentation: `UPPERCASE.md` for main docs, `lowercase.md` for specific docs
- Audio files: `descriptive_name_16khz.wav` (processed), original names preserved in raw/

## 🐍 Python Code Standards

### Style Guidelines
- Follow PEP 8 strictly with Black formatter (88 character line limit)
- Use type hints for all function parameters and return values
- Use docstrings for all classes and functions (Google style)
- Import order: standard library, third-party, local imports
- Use specific exception types, never bare `except:`

### Code Structure Template
```python
"""Module docstring describing TTS pipeline component."""

import os
import sys
from pathlib import Path
from typing import Dict, List, Optional, Union, Tuple

import numpy as np
import torch
import librosa
import soundfile as sf
from TTS.api import TTS

from .base import BaseTTSEngine
from ..utils.audio import normalize_audio, validate_audio_file


class XTTSInferenceEngine(BaseTTSEngine):
    """XTTS v2 inference engine for Singaporean accent generation.
    
    Uses few-shot voice cloning from reference audio clips to generate
    speech with authentic Singaporean accent characteristics including
    phonetic substitutions and prosodic patterns.
    """
    
    def __init__(self, model_path: str, device: str = "auto") -> None:
        """Initialize XTTS inference engine.
        
        Args:
            model_path: Path to XTTS model files
            device: Target device ("cuda", "cpu", or "auto" for detection)
            
        Raises:
            FileNotFoundError: If model files don't exist
            RuntimeError: If model loading fails
        """
        self.device = self._detect_device(device)
        self.model = self._load_model(model_path)
        self.speaker_embedding = None
    
    def build_speaker_embedding(
        self, 
        reference_clips: List[Path], 
        min_clips: int = 3
    ) -> np.ndarray:
        """Build speaker embedding from Singaporean accent reference clips.
        
        Analyzes 3-10 reference audio clips to extract voice characteristics
        including accent patterns, prosody, and rhythm for voice cloning.
        
        Args:
            reference_clips: List of paths to reference audio files
            min_clips: Minimum number of clips required for stable embedding
            
        Returns:
            Speaker embedding vector for voice cloning
            
        Raises:
            ValueError: If insufficient clips or invalid audio format
            AudioProcessingError: If embedding extraction fails
        """
        if len(reference_clips) < min_clips:
            raise ValueError(f"Need at least {min_clips} clips, got {len(reference_clips)}")
        
        # Implementation details...
        pass
    
    def generate_speech(
        self, 
        text: str, 
        temperature: float = 0.7,
        seed: Optional[int] = None
    ) -> Tuple[np.ndarray, int]:
        """Generate Singaporean-accented speech from text.
        
        Args:
            text: Input text to synthesize
            temperature: Prosody variation control (0.1-1.0)
            seed: Random seed for deterministic generation
            
        Returns:
            Tuple of (audio_array, sample_rate)
            
        Raises:
            RuntimeError: If generation fails
            ValueError: If speaker embedding not built
        """
        pass
```

### Error Handling Standards
- Use specific exception types with descriptive messages
- Log errors with appropriate levels (DEBUG, INFO, WARNING, ERROR, CRITICAL)
- Implement graceful fallbacks (XTTS → Tortoise → Error)
- Return meaningful error responses in API endpoints

### Configuration Management
- Store paths in environment variables: `ACCENT_TTS_DATA_DIR`, `ACCENT_TTS_MODEL_DIR`
- Use `config/settings.json` for hyperparameters and model settings
- Validate configuration on startup with clear error messages
- Provide sensible defaults for all optional settings

## 🎵 Audio Processing Standards

### Audio Specifications
- **Sample Rate**: 16kHz (standard for TTS models)
- **Format**: Mono WAV files for processing
- **Bit Depth**: 16-bit for storage, 32-bit float for processing
- **Loudness**: Normalized to -23 LUFS (broadcast standard)
- **Segment Length**: 1-10 seconds (target 3-8s for optimal TTS performance)

### Enhanced Audio Processing Pipeline

The enhanced pipeline provides production-grade preprocessing for multi-speaker audio with background noise:

```
Raw Audio (MP3/WAV/OGG)
    ↓
[1] Speaker Diarization (pyannote.audio)
    ↓ Identify and separate individual speakers
[2] Voice Activity Detection (silero-vad)
    ↓ Remove silence, music, background noise
[3] Noise Reduction (noisereduce)
    ↓ Clean up remaining background noise
[4] Quality Filtering (SNR, clipping, frequency)
    ↓ Reject poor quality segments
[5] Smart Segmentation (natural pauses)
    ↓ Split on phrase boundaries, not mid-word
[6] Duration Filtering (1-10 seconds)
    ↓ Keep optimal length segments
[7] Normalization (-23 LUFS)
    ↓ Consistent volume across all segments
[8] Export + Statistics
    ↓ Organized by speaker with quality report
High-Quality Training Segments
```

### Pipeline Module Responsibilities

#### 1. Speaker Diarization (`diarization.py`)
```python
class SpeakerDiarizer:
    """Identify and separate speakers using pyannote.audio.
    
    Features:
    - Automatic speaker detection (1-10 speakers)
    - Speaker labeling and tracking
    - Overlapping speech handling
    - Per-speaker segment extraction
    
    Requirements:
    - Hugging Face token for pyannote.audio models
    - Accept terms at: huggingface.co/pyannote/speaker-diarization
    """
    
    def diarize(self, audio_path: Path) -> Dict[str, List[Tuple[float, float]]]:
        """Identify speakers and return time segments.
        
        Returns:
            Dict mapping speaker_id to list of (start_time, end_time) tuples
        """
        pass
```

#### 2. Voice Activity Detection (`vad.py`)
```python
class VoiceActivityDetector:
    """Detect speech vs non-speech using silero-vad.
    
    Features:
    - Fast, accurate speech detection
    - Configurable aggressiveness (0.0-1.0)
    - Removes silence, music, background noise
    - Preserves natural speech boundaries
    
    Thresholds:
    - Min speech duration: 0.5s
    - Min silence duration: 0.3s
    - Confidence threshold: 0.5 (default)
    """
    
    def detect_speech(self, audio: np.ndarray, sr: int) -> List[Tuple[float, float]]:
        """Detect speech segments in audio.
        
        Returns:
            List of (start_time, end_time) tuples for speech regions
        """
        pass
```

#### 3. Quality Filtering (`quality_filter.py`)
```python
class AudioQualityFilter:
    """Filter segments by quality metrics.
    
    Quality Thresholds:
    - SNR (Signal-to-Noise Ratio): >15 dB
    - Clipping: <1% of samples
    - Frequency range: 80Hz - 8kHz (speech spectrum)
    - Energy variance: >0.1 (reject monotone)
    - Duration: 1-10 seconds
    
    Metrics Computed:
    - SNR using noise floor estimation
    - Clipping ratio (samples at ±1.0)
    - Spectral centroid and bandwidth
    - RMS energy and variance
    """
    
    def filter_segment(self, audio: np.ndarray, sr: int) -> Tuple[bool, Dict[str, float]]:
        """Evaluate segment quality.
        
        Returns:
            (passes_filter, quality_metrics)
        """
        pass
```

#### 4. Smart Segmentation (`segmentation.py`)
```python
class SmartSegmenter:
    """Segment audio on natural pauses, not fixed durations.
    
    Features:
    - Detect pauses >300ms
    - Avoid mid-word cuts
    - Target 3-8 second segments
    - Merge very short segments (<1s)
    - Preserve prosodic units
    
    Algorithm:
    1. Detect silence regions (VAD)
    2. Identify natural boundaries
    3. Split on pauses while respecting duration targets
    4. Merge fragments to meet minimum duration
    """
    
    def segment(self, audio: np.ndarray, sr: int) -> List[np.ndarray]:
        """Split audio on natural pauses.
        
        Returns:
            List of audio segments (1-10s each)
        """
        pass
```

### Configuration System

All pipeline parameters are configurable via `config/processing_config.yaml`:

```yaml
# Enhanced Audio Processing Configuration

diarization:
  enabled: true
  min_speakers: 1
  max_speakers: 10
  min_segment_duration: 1.0  # seconds
  
vad:
  enabled: true
  threshold: 0.5              # 0.0-1.0, higher = more aggressive
  min_speech_duration: 0.5    # seconds
  min_silence_duration: 0.3   # seconds
  
noise_reduction:
  enabled: true
  stationary: true            # Assume stationary noise
  prop_decrease: 0.8          # 0.0-1.0, reduction strength
  
quality_filter:
  min_snr: 15.0               # dB
  max_clipping_ratio: 0.01    # 1%
  min_frequency: 80           # Hz
  max_frequency: 8000         # Hz
  min_energy_variance: 0.1    # Reject flat audio
  
segmentation:
  min_duration: 1.0           # seconds
  max_duration: 10.0          # seconds
  target_duration: 5.0        # seconds (preferred)
  pause_threshold: 0.3        # seconds
  merge_short_segments: true
  
normalization:
  target_loudness: -23.0      # LUFS (broadcast standard)
  sample_rate: 16000          # Hz
  mono: true
  
output:
  format: "wav"
  bit_depth: 16
  export_rejected: false      # Save rejected segments for review
  generate_report: true       # Create quality statistics
```

### Enhanced Pipeline Usage

```python
from src.data.enhanced_processor import EnhancedAudioProcessor

# Initialize with configuration
processor = EnhancedAudioProcessor(
    config_path="config/processing_config.yaml"
)

# Process single file
results = processor.process_file(
    input_path="data/raw/interview.mp3",
    output_dir="data/processed/"
)

# Process directory (batch)
stats = processor.process_directory(
    input_dir="data/raw/realtalk-ep17/",
    output_dir="data/processed/"
)

# Output structure:
# data/processed/
# ├── speaker_0/
# │   ├── interview_seg001.wav
# │   ├── interview_seg002.wav
# │   └── ...
# ├── speaker_1/
# │   └── ...
# └── quality_report.json
```

### Quality Metrics & Reporting

The pipeline generates comprehensive statistics:

```json
{
  "input_file": "interview.mp3",
  "total_duration": 3600.0,
  "speakers_detected": 3,
  "segments_generated": 1847,
  "segments_rejected": 423,
  "quality_stats": {
    "avg_snr": 18.5,
    "avg_duration": 4.2,
    "clipping_rate": 0.003,
    "usable_audio_duration": 2891.4
  },
  "speaker_distribution": {
    "speaker_0": 892,
    "speaker_1": 654,
    "speaker_2": 301
  },
  "rejection_reasons": {
    "low_snr": 187,
    "clipping": 45,
    "too_short": 98,
    "too_long": 23,
    "low_energy": 70
  }
}
```

### Performance Characteristics

**Processing Time** (12 hours of raw audio):
- Diarization: 2-3 hours (GPU) or 6-8 hours (CPU)
- VAD + Quality: 1-2 hours
- Total: 3-10 hours depending on hardware

**Expected Yield** (from 12 hours raw):
- After VAD: 8-10 hours (removing silence/music)
- After quality filter: 6-8 hours (removing poor segments)
- Final segments: 1000-2000 per speaker (3-8s each)
- Quality improvement: 85-95% vs 60-70% with basic processing

**Resource Requirements**:
- Memory: 4-8GB RAM recommended
- Storage: 2-3x raw audio size (raw + processed + rejected)
- GPU: Optional but speeds up diarization 3-4x

### Quality Validation Standards

#### Automatic Rejection Criteria
- **SNR < 15 dB**: Too noisy for clear speech
- **Clipping > 1%**: Distorted audio
- **Duration < 1s or > 10s**: Outside optimal range
- **Frequency anomalies**: Missing speech spectrum
- **Low energy variance**: Monotone or silent

#### Manual Review Flags
- Overlapping speech detected
- Music/background noise present
- Unusual prosody patterns
- Accent inconsistencies

### Multi-Speaker Processing

The pipeline automatically:
1. Identifies all speakers in audio
2. Separates into individual tracks
3. Processes each speaker independently
4. Organizes output by speaker ID
5. Generates per-speaker quality reports

**Speaker Selection Strategies**:
- Use all speakers for diverse training data
- Select primary speaker (most segments)
- Filter by quality score threshold

## 🤖 TTS Engine Standards

### Model Architecture Decisions
- **Primary**: XTTS v2 (faster inference, good few-shot performance)
- **Fallback**: Tortoise-TTS (higher quality, slower generation)
- **Device Strategy**: Auto-detect CUDA/MPS, graceful CPU fallback
- **Memory Management**: Lazy loading, model unloading for resource efficiency

### Voice Cloning Pipeline
```python
class BaseTTSEngine:
    """Abstract base class for TTS engines."""
    
    def build_speaker_embedding(self, clips: List[Path]) -> np.ndarray:
        """Extract speaker characteristics from reference clips."""
        raise NotImplementedError
    
    def generate_speech(self, text: str, **kwargs) -> Tuple[np.ndarray, int]:
        """Generate speech with cloned voice."""
        raise NotImplementedError
    
    def get_model_info(self) -> Dict[str, Any]:
        """Return model metadata and capabilities."""
        raise NotImplementedError
```

### Performance Targets
- **XTTS Generation**: <5 seconds for 10-word sentence on CPU
- **Tortoise Generation**: <30 seconds for 10-word sentence on CPU
- **GPU Acceleration**: 3-5x speedup when available
- **Memory Usage**: <2GB RAM for XTTS, <4GB for Tortoise
- **Audio Quality**: 16kHz, clear speech, minimal artifacts

## 🌐 Gradio UI Standards

### Interface Design Principles
- **Simplicity**: Clear, intuitive controls for non-technical users
- **Responsiveness**: Works on desktop and mobile browsers
- **Accessibility**: Proper labels, keyboard navigation, screen reader support
- **Error Handling**: Clear error messages and recovery suggestions

### UI Component Structure
```python
def create_gradio_interface() -> gr.Interface:
    """Create Gradio web interface for TTS generation.
    
    Components:
    - Reference folder picker (data/processed/segments/)
    - Text input with Singlish example sentences
    - Model selector (XTTS/Tortoise)
    - Generation parameters (temperature, seed)
    - Audio output player with download
    - Generation log and error display
    """
    pass
```

### Pre-filled Test Sentences
```python
SINGLISH_EXAMPLES = [
    "The weather today is quite hot, lah.",
    "Can you help me with this thing or not?",
    "I think we should go there first, then see how.",
    "Wah, this food very nice sia!",
    "Don't worry, everything will be okay one.",
    "You want to go shopping with me anot?",
]
```

## 📊 Evaluation Framework Standards

### Phonetic Accent Markers
- **Consonant Substitutions**: "th→t/d" (thing→ting, that→dat)
- **Vowel Shifts**: TRAP-BATH split, monophthongization
- **Final Consonant Deletion**: "and→an", "want→wan"
- **Syllable-Timed Rhythm**: Equal stress on syllables vs stress-timed English

### Evaluation Checklist Template
```markdown
## Singaporean Accent Authenticity Checklist

### Phonetic Features (1-5 scale)
- [ ] TH-stopping: "th" → "t/d" substitution present
- [ ] Final consonant deletion: Natural reduction patterns
- [ ] Vowel system: Singaporean English vowel inventory
- [ ] Rhythm: Syllable-timed vs stress-timed patterns

### Prosodic Features (1-5 scale)  
- [ ] Intonation: Rising final intonation patterns
- [ ] Stress patterns: Even syllable prominence
- [ ] Speech rate: Natural conversational pace
- [ ] Pause patterns: Appropriate phrase boundaries

### Overall Quality (1-5 scale)
- [ ] Naturalness: Sounds like native Singaporean speaker
- [ ] Intelligibility: Clear and understandable
- [ ] Consistency: Stable accent throughout utterance
- [ ] Authenticity: Recognizable as Singaporean accent
```

## 🔒 Security & Privacy Standards

### Data Protection
- **Local Processing**: All TTS generation happens locally, no external APIs
- **Audio Privacy**: Never log or store user-generated audio without consent
- **Reference Audio**: Store in gitignored directories with proper permissions
- **Model Security**: Validate model files, check for tampering

### Environment Variables
```bash
# .env.example
ACCENT_TTS_DATA_DIR=./data
ACCENT_TTS_MODEL_DIR=./models
ACCENT_TTS_OUTPUT_DIR=./outputs
ACCENT_TTS_LOG_LEVEL=INFO
ACCENT_TTS_DEVICE=auto
ACCENT_TTS_MAX_AUDIO_LENGTH=300  # seconds
```

### File Permissions
- Audio files: 600 (owner read/write only)
- Model files: 644 (owner write, group/other read)
- Configuration: 644 (readable for debugging)
- Logs: 640 (owner write, group read)

## 🧪 Testing Standards

### Test Organization
- Unit tests: `tests/unit/test_module_name.py`
- Integration tests: `tests/integration/test_pipeline_name.py`
- Audio tests: `tests/audio/` with sample files
- Use pytest with fixtures for audio data

### Test Categories
```python
def test_audio_processing_pipeline():
    """Test complete audio processing from raw to segments."""
    pass

def test_xtts_inference_with_reference_clips():
    """Test XTTS generation with Singaporean accent clips."""
    pass

def test_gradio_interface_functionality():
    """Test UI components and user interactions."""
    pass

def test_accent_evaluation_metrics():
    """Test phonetic and prosodic feature detection."""
    pass
```

### Audio Test Data
- Include small sample clips for automated testing
- Use synthetic/public domain audio to avoid copyright issues
- Test edge cases: silence, noise, different formats
- Validate against known good outputs

## 🔄 Git Workflow & Automation

### Repository Security
- Comprehensive `.gitignore` excluding all audio files and models
- Never commit API keys, personal audio, or sensitive data
- Use Git LFS for large model files (>100MB) if needed
- Regular security audits of committed files

### Commit Message Format
```
feat(xtts): add speaker embedding generation
fix(audio): resolve segmentation boundary detection
docs(readme): update installation instructions for macOS
test(integration): add end-to-end TTS pipeline tests
refactor(ui): improve Gradio interface responsiveness
```

### Makefile Automation
```makefile
.PHONY: setup prepare ui demo clean test lint

setup:          # Create venv, install deps, check ffmpeg
prepare:        # Run ingest→prepare→transcribe pipeline  
ui:             # Launch Gradio interface on localhost:7860
demo:           # Generate demo.wav with sample Singlish text
clean:          # Remove processed files and outputs
test:           # Run pytest suite with coverage
lint:           # Run black, flake8, mypy checks
```

## 📈 Development Phases & Milestones

### Phase 1: Foundation ✅ CURRENT
- [x] Project structure and documentation
- [x] Environment setup and dependency management
- [ ] Audio processing pipeline (ingest→prepare→transcribe)
- [ ] Basic XTTS integration and testing

### Phase 2: Core TTS Implementation
- [ ] XTTS v2 inference engine with speaker embedding
- [ ] Tortoise-TTS fallback system
- [ ] Voice cloning pipeline with reference clips
- [ ] Audio quality validation and metrics

### Phase 3: User Interface & Experience
- [ ] Gradio web interface with all controls
- [ ] Pre-filled Singlish example sentences
- [ ] Real-time generation progress and error handling
- [ ] Audio playback and download functionality

### Phase 4: Evaluation & Polish
- [ ] Comprehensive accent evaluation framework
- [ ] Automated testing suite with audio samples
- [ ] Performance optimization and GPU acceleration
- [ ] Documentation and deployment guides

## 🎯 Success Criteria

### Technical Requirements
- `make setup` completes without errors on macOS
- `make ui` launches functional Gradio interface
- `make demo` generates recognizable Singaporean-accented speech
- End-to-end pipeline processes reference clips to speech output
- Both XTTS and Tortoise engines produce quality results

### Quality Benchmarks
- Generated speech exhibits clear Singaporean accent markers
- Audio quality suitable for conversational applications
- Processing time reasonable for interactive use (<30s per sentence)
- System handles various input text lengths and complexity
- Evaluation framework provides meaningful accent assessment

### User Experience Goals
- Non-technical users can generate accented speech easily
- Clear error messages and recovery guidance
- Responsive interface works across devices and browsers
- Comprehensive documentation enables quick setup
- Example sentences demonstrate system capabilities effectively

## 🔧 Development Environment

### Python Environment
- Python 3.10+ (tested on 3.11)
- Use `venv` for virtual environment management
- Install PyTorch with CUDA support if available
- Pin dependency versions for reproducibility

### System Dependencies
- FFmpeg for audio processing and format conversion
- Git LFS for large model files (if needed)
- Sufficient disk space for models and audio data (5-10GB)
- GPU recommended but not required (CPU fallback available)

### IDE Configuration
- Use type checking (mypy) and formatting (black)
- Configure pytest for test discovery and coverage
- Set up debugging for audio processing pipelines
- Enable linting for code quality maintenance

## 📞 Contact & Maintenance
- Owner: Tom Yan
- Email: zhiyuanyan@rochester.edu  
- GitHub: tomyanzhiyuan
- Project: Singaporean-Accent TTS MVP
- Last Updated: September 2024
